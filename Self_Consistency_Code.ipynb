{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pacozaa/LLM-Paper-To-Code/blob/main/Self_Consistency_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAeJBuOU5o2O"
      },
      "source": [
        "We are utilizing free tier of GitHub Models.\n",
        "\n",
        "\n",
        "Check out the billing condition here but you should have a free tier if you have GitHub account:\n",
        "\n",
        "https://docs.github.com/en/github-models/use-github-models/prototyping-with-ai-models#rate-limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCp88fVHLRQI",
        "outputId": "dcbde2f8-65ca-4f4d-dd53-78a0e18628ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: azure-ai-inference in /usr/local/lib/python3.12/dist-packages (1.0.0b9)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from azure-ai-inference) (0.7.2)\n",
            "Requirement already satisfied: azure-core>=1.30.0 in /usr/local/lib/python3.12/dist-packages (from azure-ai-inference) (1.36.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from azure-ai-inference) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "#Install Dependency\n",
        "!pip install azure-ai-inference datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "zMgjlCStPjGl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage, AssistantMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from collections import Counter\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iadGSMh1mNx"
      },
      "source": [
        "We are using one of the puzzle from **ZebraLogic Bench** for this experiment\n",
        "\n",
        "- Dataset:\n",
        "https://huggingface.co/datasets/allenai/ZebraLogicBench-private/viewer/grid_mode/test?row=0&views%5B%5D=grid_mode\n",
        "- Dataset Article:\n",
        "https://huggingface.co/blog/yuchenlin/zebra-logic\n",
        "\n",
        "- Find the example row by this sql in huggingface dataset studio\n",
        "```sql\n",
        "SELECT *\n",
        "FROM grid_mode\n",
        "WHERE id = 'lgp-test-3x3-24'\n",
        "LIMIT 10;\n",
        "```\n",
        "- or use this link: https://huggingface.co/datasets/allenai/ZebraLogicBench-private/viewer/grid_mode/test?f%5Bsize%5D%5Bvalue%5D=%273*3%27&row=41&views%5B%5D=grid_mode&sql=SELECT+*+%0A++FROM+grid_mode+%0A++WHERE+id+%3D+%27lgp-test-3x3-24%27%0A++LIMIT+10%3B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QMNCOhi69KR"
      },
      "source": [
        "# Start Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12UoDnpHv9DO"
      },
      "source": [
        "# 1. Set Up Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "DHqpmeskdpBx"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#GitHub Token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZcA8dQgvw3z"
      },
      "source": [
        "# 2. Prepare Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "ISwTHR60NQJs"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "# CoT Prompt\n",
        "system_prompt=\"\"\"\n",
        " Each example/input/question is a Logic Grid Puzzle, also known as a Zebra Puzzle. In each puzzle, we are given N houses (numbered 1 to N from left to right) and M features for each house. There are N distinct values for each feature, and each house must have a unique value for each feature. Given a list of clues, one should be able to deduce a unique correct assignment of values. The logic grid puzzle is a typical Constraint Satisfaction Problem (CSP) and is often used to test humans' logical reasoning abilities in exams such as the Law School Admission Test (LSAT).\n",
        "\n",
        "You always solve problem step by step\n",
        "start answering with `## Reasoning steps:` and end with\n",
        "\n",
        "## Final answer:\n",
        "```json\n",
        "\n",
        "````\n",
        "\n",
        "NB. Always use double quote inside json block\n",
        "\"\"\"\n",
        "# Example Input Prompt\n",
        "user_prompt=\"\"\"\n",
        "There are 2 houses, numbered 1 to 2 from left to right.\n",
        "Each house is occupied by a different person.\n",
        "Each house has a unique attribute for each of the following characteristics:\n",
        "- Each person has a unique name: **Arnold, Eric**\n",
        "- People own unique car models: **ford f150, tesla model 3**\n",
        "- The people keep unique animals: **cat, horse**\n",
        "\n",
        "**Clues**:\n",
        "1. Eric is directly left of the person who owns a Tesla Model 3.\n",
        "2. The person who keeps horses is in the first house.\n",
        "\"\"\"\n",
        "# Example Output Prompt\n",
        "assistant_message=\"\"\"\n",
        "## Reasoning steps:\n",
        "\n",
        "From Clue 1, we know that Eric is to the left of someone, so he must be the owner of House 1 because House 2 is the rightmost house.\n",
        "Additionally, we know that the person in House 2 must be Arnold, and he owns a Tesla Model 3. Thus, Eric owns a Ford F150.\n",
        "From Clue 2, we know that Eric keeps horses in House 1, which means the other house must keep cats. Finally, we arrive at the unique solution to this puzzle.\n",
        "The solution is presented in table format:\n",
        "\n",
        "## Final answer:\n",
        "\n",
        "```json\n",
        "{\n",
        "   \"header\":[\n",
        "      \"Houses\",\n",
        "      \"Name\",\n",
        "      \"CarModel\",\n",
        "      \"Animal\"\n",
        "   ],\n",
        "   \"rows\":[\n",
        "      [\n",
        "         \"1\",\n",
        "         \"Eric\",\n",
        "         \"ford f150\",\n",
        "         \"horse\"\n",
        "      ],\n",
        "      [\n",
        "         \"2\",\n",
        "         \"Arnold\",\n",
        "         \"tesla model 3\",\n",
        "         \"cat\"\n",
        "      ]\n",
        "   ]\n",
        "}\n",
        "```\n",
        "\"\"\"\n",
        "# lgp-test-2x2-33\n",
        "# lgp-test-6x5-2\n",
        "# lgp-test-2x4-33\n",
        "# lgp-test-6x6-5\n",
        "# lgp-test-3x3-24\n",
        "question=\"\"\"\n",
        "Question:\n",
        "There are 3 houses, numbered 1 to 3 from left to right, as seen from across the street. Each house is occupied by a different person. Each house has a unique attribute for each of the following characteristics:\n",
        "- Each person has a unique name: `Peter`, `Eric`, `Arnold`\n",
        "- Each person has a favorite color: `red`, `white`, `yellow`\n",
        "- Each mother is accompanied by their child: `Fred`, `Meredith`, `Bella`\n",
        "\n",
        "## Clues:\n",
        "1. Arnold is the person whose favorite color is red.\n",
        "2. The person's child is named Fred is somewhere to the left of Eric.\n",
        "3. The person whose favorite color is red is in the second house.\n",
        "4. The person's child is named Bella is in the first house.\n",
        "5. The person who loves white is the person's child is named Meredith.\n",
        "\n",
        "## Headers\n",
        "\"header\": [\n",
        "\"House\",\n",
        "\"Name\",\n",
        "\"Color\",\n",
        "\"Children\"\n",
        "]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvIGZFmBwF59"
      },
      "source": [
        "# 3. Construct Messages with Few Shot Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_fGsMg3wM8N"
      },
      "source": [
        "# 4. Extract Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "Of0UbqVwTmtI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_final_answer_regex(answer):\n",
        "    match = re.search(r'## Final answer:\\s*(.*)', answer, re.DOTALL | re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    return None\n",
        "\n",
        "def extract_json_from_string(input_string):\n",
        "    \"\"\"\n",
        "    Extract JSON from a string that contains ```json``` code blocks.\n",
        "\n",
        "    Args:\n",
        "        input_string (str): Input string that may contain JSON code blocks\n",
        "\n",
        "    Returns:\n",
        "        list: List of parsed JSON objects/dictionaries, or empty list if none found\n",
        "    \"\"\"\n",
        "    # Regular expression pattern to match JSON code blocks\n",
        "    pattern = r'```json\\n(.*?)\\n```'\n",
        "\n",
        "    # Find all matches in the input string\n",
        "    matches = re.findall(pattern, input_string, re.DOTALL)\n",
        "\n",
        "    json_objects = []\n",
        "\n",
        "    for match in matches:\n",
        "        try:\n",
        "            # Parse the JSON string into a Python object\n",
        "            json_obj = json.loads(match.strip())\n",
        "            json_objects.append(json_obj)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error parsing JSON: {e}\")\n",
        "            print(f\"Problematic JSON string: {match}\")\n",
        "            continue\n",
        "\n",
        "    return json_objects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fafMwqIqwRGn"
      },
      "source": [
        "# 5. Construct LLM Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "g5BzPnE3ueBC"
      },
      "outputs": [],
      "source": [
        "github_token=userdata.get('GITHUB_TOKEN')\n",
        "\n",
        "endpoint = \"https://models.github.ai/inference\"\n",
        "\n",
        "client = ChatCompletionsClient(\n",
        "    endpoint=endpoint,\n",
        "    credential=AzureKeyCredential(github_token),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKF3QEIDwjyp"
      },
      "source": [
        "# 6. Normalize and Compare Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "MFCz_BrPgRa2"
      },
      "outputs": [],
      "source": [
        "def find_name_column_index(answer):\n",
        "    \"\"\"Helper function to find the name column index in the header\"\"\"\n",
        "    if 'header' in answer:\n",
        "        try:\n",
        "            return answer['header'].index('name')\n",
        "        except (ValueError, IndexError):\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def normalize_answer(answer, for_comparison=False):\n",
        "    \"\"\"\n",
        "    Unified function to normalize answers for either counting or comparison.\n",
        "\n",
        "    Args:\n",
        "        answer: The answer to normalize\n",
        "        for_comparison: If True, returns normalized rows for comparison.\n",
        "                       If False, returns JSON string for counting duplicates.\n",
        "    \"\"\"\n",
        "    if isinstance(answer, dict) and 'rows' in answer:\n",
        "        name_idx = find_name_column_index(answer)\n",
        "\n",
        "        if for_comparison:\n",
        "            # For comparison: normalize all values and sort rows\n",
        "            normalized_rows = []\n",
        "            for row in answer['rows']:\n",
        "                if name_idx is not None:\n",
        "                    # Use the found name column index\n",
        "                    normalized_row = []\n",
        "                    for i, value in enumerate(row):\n",
        "                        normalized_row.append(str(value).replace(\" \", \"\").lower())\n",
        "                    normalized_rows.append(tuple(normalized_row))\n",
        "                else:\n",
        "                    # Fallback: normalize all values\n",
        "                    normalized_rows.append(tuple(str(val).replace(\" \", \"\").lower() for val in row))\n",
        "\n",
        "            # Sort rows by name for consistent comparison\n",
        "            normalized_rows.sort()\n",
        "            return normalized_rows\n",
        "        else:\n",
        "            # For counting: create dictionary keyed by name\n",
        "            rows_by_name = {}\n",
        "            for row in answer['rows']:\n",
        "                if name_idx is not None:\n",
        "                    name = row[name_idx]\n",
        "                    rows_by_name[name] = tuple(row)  # Convert to tuple for hashability\n",
        "                else:\n",
        "                    # If no name column found, use second column (common pattern) or whole row\n",
        "                    if len(row) > 1:\n",
        "                        rows_by_name[row[1]] = tuple(row)\n",
        "                    else:\n",
        "                        rows_by_name[str(row)] = tuple(row)\n",
        "            return json.dumps(rows_by_name, sort_keys=True)\n",
        "    else:\n",
        "        if for_comparison:\n",
        "            return str(answer).replace(\" \", \"\").lower()\n",
        "        else:\n",
        "            return str(answer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqGqEPhHwYfN"
      },
      "source": [
        "# 7. Call LLM in loop to get samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "collapsed": true,
        "id": "1L6GqRnHLKkl"
      },
      "outputs": [],
      "source": [
        "def generate_and_extract_answers(client, messages, num_samples, temperature, top_p, model_name):\n",
        "    \"\"\"\n",
        "    Generate multiple samples and extract final answers from the responses.\n",
        "\n",
        "    Args:\n",
        "        client: The API client object\n",
        "        messages: List of messages for the conversation\n",
        "        num_samples: Number of samples to generate\n",
        "        temperature: Temperature for sampling\n",
        "        top_p: Top-p for sampling\n",
        "        model_name: Name of the model to use\n",
        "\n",
        "    Returns:\n",
        "        list: List of extracted final answers\n",
        "    \"\"\"\n",
        "    final_answers = []  # Collect all final answers\n",
        "\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        print(f\"\\nüìù Generating sample {i+1}/{num_samples}...\")\n",
        "        # time.sleep(1)\n",
        "        response = client.complete(\n",
        "            messages=messages,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            model=model_name,\n",
        "        )\n",
        "        try:\n",
        "          answer = response.choices[0].message.content\n",
        "          generator_output_extracted = extract_final_answer_regex(answer)\n",
        "          print(f\"‚úÖ Received response for sample {i+1}\")\n",
        "\n",
        "          if generator_output_extracted is None:\n",
        "            print(f\"‚ùå Warning: Could not extract answer from sample(Index: {i})\")\n",
        "\n",
        "          else:\n",
        "            clean_generator_output_extracted = generator_output_extracted.lower().replace(\" \", \"\")\n",
        "\n",
        "            json_object_generator_output_extracted = extract_json_from_string(clean_generator_output_extracted)\n",
        "\n",
        "            final_answers.append(json_object_generator_output_extracted[0])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sample {i+1}: {e}\")\n",
        "\n",
        "    return final_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cJm-Plewn3b"
      },
      "source": [
        "# 8. Get final answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "FmhLDLCntXb2"
      },
      "outputs": [],
      "source": [
        "def analyze_and_display_answers(final_answers, num_samples):\n",
        "    if not final_answers:\n",
        "        return\n",
        "\n",
        "    # Convert dicts to JSON strings for counting\n",
        "    json_strings = [json.dumps(answer, sort_keys=True) for answer in final_answers]\n",
        "    counter = Counter(json_strings)\n",
        "\n",
        "    # Get most common JSON string\n",
        "    most_common_json, count = counter.most_common(1)[0]\n",
        "\n",
        "    # Find the original dict that matches\n",
        "    for answer in final_answers:\n",
        "        if json.dumps(answer, sort_keys=True) == most_common_json:\n",
        "            most_common_dict = answer\n",
        "            break\n",
        "\n",
        "    print(f\"Most common answer (appears {count} times):\")\n",
        "\n",
        "    return most_common_dict, count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38zLWdj9wsGT"
      },
      "source": [
        "# 9. Check Correct Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "c3DZsLXSwz6D"
      },
      "outputs": [],
      "source": [
        "def compare_answers(correct_answer, most_common_answer, final_answers):\n",
        "    \"\"\"\n",
        "    Compare the most common answer and all final answers against the correct answer.\n",
        "\n",
        "    Args:\n",
        "        correct_answer: The ground truth correct answer\n",
        "        most_common_answer: The most frequently occurring answer\n",
        "        final_answers: List of all answers to check\n",
        "\n",
        "    Returns:\n",
        "        dict: Comparison results including matches and normalized answers\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize the correct answer\n",
        "    correct_normalized = normalize_answer(correct_answer, for_comparison=True)\n",
        "\n",
        "    # Normalize the most common answer\n",
        "    most_common_normalized = normalize_answer(most_common_answer, for_comparison=True)\n",
        "\n",
        "\n",
        "    # Check if most common answer matches correct answer\n",
        "    most_common_matches = most_common_normalized == correct_normalized\n",
        "\n",
        "    # print(f\"{'='*60}\")\n",
        "    # print(f\"ans:{most_common_normalized}\")\n",
        "    # print(f\"cor:{correct_normalized}\")\n",
        "    # print(f\"{'='*60}\")\n",
        "\n",
        "    if most_common_matches:\n",
        "        print(\"‚úÖ SUCCESS: The most common answer is correct!\")\n",
        "        matching_answers = [i for i, answer in enumerate(final_answers)\n",
        "                          if normalize_answer(answer, for_comparison=True) == correct_normalized]\n",
        "    else:\n",
        "        print(\"‚ùå The most common answer does NOT match the correct answer\")\n",
        "        print(\"\\nChecking if any answer in final_answers matches the correct answer...\")\n",
        "\n",
        "        # Check all answers for matches\n",
        "        matching_answers = []\n",
        "        for i, answer in enumerate(final_answers):\n",
        "            answer_normalized = normalize_answer(answer, for_comparison=True)\n",
        "            if answer_normalized == correct_normalized:\n",
        "                matching_answers.append(i)\n",
        "\n",
        "        if matching_answers:\n",
        "            print(f\"‚úÖ FOUND {len(matching_answers)} MATCHING ANSWER(S) at indices: {matching_answers}\")\n",
        "            # print(f\"First matching answer: {final_answers[matching_answers[0]]}\")\n",
        "        else:\n",
        "            print(\"‚ùå No answers in final_answers match the correct answer\")\n",
        "\n",
        "    # Return results for programmatic use\n",
        "    return {\n",
        "        'most_common_matches': most_common_matches,\n",
        "        'matching_answers_indices': matching_answers,\n",
        "        'correct_normalized': correct_normalized,\n",
        "        'most_common_normalized': most_common_normalized,\n",
        "        'matching_answers': [final_answers[i] for i in matching_answers] if matching_answers else []\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "iKcUkT1xVkGJ"
      },
      "outputs": [],
      "source": [
        "# Run all together\n",
        "def runSC(question, solution, num_samples,temperature,top_p,model_name):\n",
        "  messages = [\n",
        "      SystemMessage(system_prompt),\n",
        "      UserMessage(user_prompt),\n",
        "      AssistantMessage(assistant_message),\n",
        "      UserMessage(question)\n",
        "  ]\n",
        "\n",
        "  final_answers = generate_and_extract_answers(client, messages, num_samples, temperature, top_p, model_name)\n",
        "\n",
        "  most_common_answer,count = analyze_and_display_answers(final_answers,num_samples)\n",
        "  compare_result = compare_answers(solution, most_common_answer, final_answers)\n",
        "  return {\n",
        "      \"most_common_answer\": most_common_answer,\n",
        "      \"most_common_matches\": compare_result[\"most_common_matches\"],\n",
        "      \"final_answers\": final_answers\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "rCbrNzWeTk7F"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "def load_puzzle(ds_size):\n",
        "  # Or, use the token from Colab secrets\n",
        "  hf_token = userdata.get('HF_TOKEN')\n",
        "  dataset = load_dataset(\"allenai/ZebraLogicBench-private\", \"grid_mode\", split=\"test\", token=hf_token)\n",
        "  grid_size = \"4*4\"\n",
        "\n",
        "  filtered_dataset = dataset.filter(lambda x: x[\"size\"] == grid_size).select(range(ds_size)).shuffle(seed=42)\n",
        "  return filtered_dataset\n",
        "  # display(filtered_dataset)\n",
        "  # https://huggingface.co/datasets/allenai/ZebraLogicBench-private\n",
        "  # Columns: id, size, puzzle, solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "pFMd3Y-pZSA_"
      },
      "outputs": [],
      "source": [
        "def main(num_samples=1,ds_size = 15):\n",
        "  is_correct_list = []\n",
        "  filtered_dataset=load_puzzle(ds_size)\n",
        "\n",
        "\n",
        "  # Get total number of items for progress tracking\n",
        "  total_items = len(filtered_dataset)\n",
        "  print(f\"Starting processing of {total_items} items...\")\n",
        "\n",
        "  model_name = \"openai/gpt-4.1-nano\" # Choose not too smart model to see the diversity (phi-4,openai/gpt-4.1,deepseek/DeepSeek-R1-0528,xai/grok-3,openai/gpt-4.1-mini,deepseek/DeepSeek-V3-0324)\n",
        "  temperature=1.0 # Choose 1.0 for the most diverse answers\n",
        "  top_p=1.0 # Choose 1.0 for the most diverse answers\n",
        "\n",
        "  for index, item in enumerate(filtered_dataset):\n",
        "      # Calculate progress percentage\n",
        "      progress = (index + 1) / total_items * 100\n",
        "\n",
        "\n",
        "      puzzle_id = item[\"id\"]\n",
        "      puzzle_input = item[\"puzzle\"]\n",
        "\n",
        "      solution = item[\"solution\"]\n",
        "\n",
        "      print(f\"\\n--- Processing item {puzzle_id}:{index + 1}/{total_items} ({progress:.1f}%) ---\")\n",
        "      solution_normalized = normalize_answer(solution, for_comparison=True)\n",
        "      full_question=f\"{puzzle_input}\\n## Headers\\n\\\"header\\\":{item[\"solution\"]['header']}\"\n",
        "      results = runSC(full_question,solution,num_samples,temperature,top_p,model_name)\n",
        "      print(f\"{'='*50}\")\n",
        "      is_correct_list.append(results['most_common_matches'])\n",
        "      # delay/sleep 1 sec\n",
        "      time.sleep(1)\n",
        "\n",
        "  # Calculate final statistics\n",
        "  correct_count = sum(1 for item in is_correct_list if item)\n",
        "  accuracy = correct_count / len(is_correct_list) * 100 if is_correct_list else 0\n",
        "\n",
        "  print(f\"\\n{'='*50}\")\n",
        "  print(f\"‚úÖ Iteration complete! Processed {len(is_correct_list)} items\")\n",
        "  print(f\"üìä Results: {correct_count}/{len(is_correct_list)} correct ({accuracy:.1f}% accuracy)\")\n",
        "  print(f\"{'='*50}\")\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUhPKw02ObtL"
      },
      "outputs": [],
      "source": [
        "results_of_12=main(num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_of_1=main(num_samples=1)"
      ],
      "metadata": {
        "id": "IyJnZMwcN2s1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBsDoYFUgyVMhy1yRwwX9t",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}